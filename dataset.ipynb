{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import boto3.session\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, Executor\n",
    "import threading\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StorageConfig = collections.namedtuple(\n",
    "    \"StorageConfig\",\n",
    "    [\n",
    "        \"endpoint_url\",\n",
    "        \"aws_access_key_id\",\n",
    "        \"aws_secret_access_key\",\n",
    "        \"region_name\",\n",
    "        \"bucket_name\",\n",
    "        \"config\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "config = StorageConfig(\n",
    "    endpoint_url=\"http://localhost:9000\",\n",
    "    aws_access_key_id=\"username\",\n",
    "    aws_secret_access_key=\"password\",\n",
    "    config=Config(signature_version=\"s3v4\"),\n",
    "    region_name=\"us-east-1\",\n",
    "    bucket_name=\"data\",\n",
    ")\n",
    "\n",
    "\n",
    "def get_bucket(w, h):\n",
    "    if w > 900:\n",
    "        bucket_w = 900\n",
    "    elif w < 120:\n",
    "        bucket_w = (w // 10 + 1) * 10\n",
    "    else:\n",
    "        bucket_w = (w // 15) * 15\n",
    "\n",
    "    if h > 45:\n",
    "        bucket_h = (h // 10) * 10\n",
    "    else:\n",
    "        bucket_h = 32\n",
    "\n",
    "    return bucket_w, bucket_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoteDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        storage_config: StorageConfig,\n",
    "        project_name: str,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.client = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=storage_config.endpoint_url,\n",
    "            aws_access_key_id=storage_config.aws_access_key_id,\n",
    "            aws_secret_access_key=storage_config.aws_secret_access_key,\n",
    "            config=storage_config.config,\n",
    "            region_name=storage_config.region_name,\n",
    "        )\n",
    "\n",
    "        self.total_count = 0\n",
    "        try:\n",
    "            logging.info(\n",
    "                f\"Fetching data information from {storage_config.endpoint_url}, please wait...\"\n",
    "            )\n",
    "            result = self.client.list_objects(\n",
    "                Bucket=storage_config.bucket_name,\n",
    "                Prefix=f\"/{project_name}/\",\n",
    "                Delimiter=\"/\",\n",
    "            )\n",
    "\n",
    "            self.buckets = {}\n",
    "            if \"CommonPrefixes\" in result:\n",
    "                # Hotfix: get first version\n",
    "                item = result[\"CommonPrefixes\"][0]\n",
    "\n",
    "                response = self.client.list_objects_v2(\n",
    "                    Bucket=storage_config.bucket_name,\n",
    "                    Prefix=item[\"Prefix\"],\n",
    "                )\n",
    "\n",
    "                keys = response[\"Contents\"]\n",
    "                while (\n",
    "                    \"NextContinuationToken\" in response\n",
    "                    and response[\"NextContinuationToken\"]\n",
    "                ):\n",
    "                    response = self.client.list_objects_v2(\n",
    "                        Bucket=storage_config.bucket_name,\n",
    "                        Prefix=item[\"Prefix\"],\n",
    "                        ContinuationToken=response[\"NextContinuationToken\"],\n",
    "                    )\n",
    "\n",
    "                    keys.extend(response[\"Contents\"])\n",
    "\n",
    "                self.cluster_indices = defaultdict(list)\n",
    "                for file in tqdm(keys):\n",
    "                    info = self.client.head_object(\n",
    "                        Bucket=storage_config.bucket_name, Key=file[\"Key\"]\n",
    "                    )\n",
    "                    bucket_w, bucket_h = get_bucket(\n",
    "                        int(info[\"Metadata\"][\"width\"]), int(info[\"Metadata\"][\"height\"])\n",
    "                    )\n",
    "                    self.cluster_indices[(bucket_w, bucket_h)].append(file[\"Key\"])\n",
    "                self.total_count = len(keys)\n",
    "        except ClientError as e:\n",
    "            logging.critical(e)\n",
    "            raise ValueError(\n",
    "                \"Something went wrong when fetching data information from {} with provided credentials\"\n",
    "            )\n",
    "        self.bucket_name = config.bucket_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_count\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        key = None\n",
    "        for data in self.cluster_indices.values():\n",
    "            index = index - len(data)\n",
    "            if index < 0:\n",
    "                key = data[index + len(data)]\n",
    "                break\n",
    "        if key is None:\n",
    "            raise IndexError(f\"Cannot get item with index {index}\")\n",
    "\n",
    "        data = self.client.get_object(Bucket=self.bucket_name, Key=key)\n",
    "        contents = data[\"Body\"].read()\n",
    "        image_np = np.frombuffer(contents, np.uint8)\n",
    "        img_np = cv2.imdecode(image_np, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return img_np\n",
    "\n",
    "    def _get_keys(self, response: dict) -> list:\n",
    "        if \"Contents\" in response:\n",
    "            return [item[\"Key\"] for item in response[\"Contents\"]]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RemoteDataset(project_name=\"online_fetching\", storage_config=config)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetDataTask(threading.Thread):\n",
    "    def run(self):\n",
    "        storage_config, keys = self._args\n",
    "        client = boto3.client(\n",
    "            \"s3\",\n",
    "            endpoint_url=storage_config.endpoint_url,\n",
    "            aws_access_key_id=storage_config.aws_access_key_id,\n",
    "            aws_secret_access_key=storage_config.aws_secret_access_key,\n",
    "            config=storage_config.config,\n",
    "            region_name=storage_config.region_name,\n",
    "        )\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = [\n",
    "                executor.submit(get_single_file, config.bucket_name, client, key)\n",
    "                for key in keys\n",
    "            ]\n",
    "\n",
    "        self.result = []\n",
    "        for f in as_completed(futures):\n",
    "            self.result.append(f.result())\n",
    "\n",
    "\n",
    "def get_single_file(bucket, client, key):\n",
    "    data = client.get_object(Bucket=bucket, Key=key)\n",
    "    contents = data[\"Body\"].read()\n",
    "    image_np = np.frombuffer(contents, np.uint8)\n",
    "    return cv2.imdecode(image_np, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, batch_size, shuffle=True):\n",
    "        super().__init__(data_source)\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_list(lst):\n",
    "        return [item for sublist in lst for item in sublist]\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_lists = []\n",
    "\n",
    "        for cluster_indices in self.data_source.cluster_indices.values():\n",
    "            if self.shuffle:\n",
    "                random.shuffle(cluster_indices)\n",
    "\n",
    "            batches = [\n",
    "                cluster_indices[i : i + self.batch_size]\n",
    "                for i in range(0, len(cluster_indices), self.batch_size)\n",
    "            ]\n",
    "            batches = [_ for _ in batches if len(_) == self.batch_size]\n",
    "\n",
    "            if self.shuffle:\n",
    "                random.shuffle(batches)\n",
    "\n",
    "            batch_lists.append(batches)\n",
    "\n",
    "        batch_lists = self.flatten_list(batch_lists)\n",
    "        if self.shuffle:\n",
    "            random.shuffle(batch_lists)\n",
    "\n",
    "        self.batch_lists = batch_lists\n",
    "        self.index = 0\n",
    "\n",
    "        self.thread = GetDataTask(args=(config, self.batch_lists[0]))\n",
    "        self.thread.start()\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index <= len(self.batch_lists):\n",
    "            self.thread.join()\n",
    "            self.index += 1\n",
    "\n",
    "            result = self.thread.result\n",
    "\n",
    "            self.thread = GetDataTask(args=(config, self.batch_lists[0]))\n",
    "            self.thread.start()\n",
    "            return result\n",
    "        else:\n",
    "            raise StopIteration\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ClusterRandomSampler(data_source=dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(sampler):\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
